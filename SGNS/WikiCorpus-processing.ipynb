{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理程序结束！\n"
     ]
    }
   ],
   "source": [
    "# 将从网络上下载的xml格式的wiki百科训练语料转为txt格式\n",
    "\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "\n",
    "input_file_name = '../SGNS/data/WikiCorpus/zhwiki-latest-pages-articles.xml.bz2'\n",
    "output_file_name = '../SGNS/data/WikiCorpus/wiki.cn.txt'\n",
    "input_file = WikiCorpus(input_file_name, lemmatize=False, dictionary={})\n",
    "output_file = open(output_file_name, 'w', encoding=\"utf-8\")\n",
    "\n",
    "count = 0\n",
    "for text in input_file.get_texts():\n",
    "    output_file.write(' '.join(text) + '\\n')\n",
    "    count = count + 1\n",
    "    if count == 5000:\n",
    "        break\n",
    "\n",
    "print('处理程序结束！')\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# 将繁体字转换成简体字\n",
    "import zhconv\n",
    "\n",
    "input_file_name = '../SGNS/data/WikiCorpus/wiki.cn.txt'\n",
    "output_file_name = '../SGNS/data/WikiCorpus/wiki.cn.simple.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "lines = input_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    output_file.write(zhconv.convert(line, 'zh-hans'))\n",
    "    \n",
    "print('转换程序执行结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "import jieba\n",
    "\n",
    "input_file_name = '../SGNS/data/WikiCorpus/wiki.cn.simple.txt'\n",
    "output_file_name = '../SGNS/data/WikiCorpus/wiki.cn.simple.separate.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "lines = input_file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    # jieba分词的结果是一个list，需要拼接，但是jieba把空格回车都当成一个字符处理\n",
    "    output_file.write(' '.join(jieba.cut(line.split('\\n')[0].replace(' ', ''))) + '\\n')\n",
    "\n",
    "print('分词程序执行结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序执行结束！\n"
     ]
    }
   ],
   "source": [
    "# 去掉英文字符\n",
    "import re\n",
    "\n",
    "input_file_name = '../SGNS/data/WikiCorpus/wiki.cn.simple.separate.txt'\n",
    "output_file_name = '../SGNS/data/WikiCorpus/wiki.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "lines = input_file.readlines()\n",
    "\n",
    "cn_reg = '^[\\u4e00-\\u9fa5]+$'\n",
    "\n",
    "for line in lines:\n",
    "    line_list = line.split('\\n')[0].split(' ')\n",
    "    line_list_new = []\n",
    "    for word in line_list:\n",
    "        if re.search(cn_reg, word):\n",
    "            line_list_new.append(word)\n",
    "    output_file.write(' '.join(line_list_new) + '\\n')\n",
    "    \n",
    "print('程序执行结束！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
